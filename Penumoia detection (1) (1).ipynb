{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69d238ae-1cd1-41b9-a17b-938590f3e0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea52071-6801-4fdd-9374-276ed5baf898",
   "metadata": {},
   "source": [
    "Pre-Processing the training test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a95a723-6ab8-4aa6-8afd-6657e0114a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.3,\n",
    "    zoom_range=0.3,\n",
    "    rotation_range=20,  \n",
    "    width_shift_range=0.2,  \n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15edf12a-63c1-44d2-bdf7-293c858ab097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5216 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_set = train_datagen.flow_from_directory(r'E:\\AI\\chest_xray\\train',\n",
    "    target_size=(64, 64),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f144662a-aa00-4ff0-a342-37d41b3aa201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 624 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen=ImageDataGenerator(rescale=1./255)\n",
    "test_set= test_datagen.flow_from_directory(r'E:\\AI\\chest_xray\\test',\n",
    "                                           target_size=(64,64),\n",
    "                                           batch_size=32,\n",
    "                                           class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ef07af-6d71-4ad3-b77a-8d88f70edf37",
   "metadata": {},
   "source": [
    "Building the CNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc0a3ae3-892a-47b8-8ac8-0b1ce3e5afa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn=tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691a7a6b-15ac-4095-bd4a-0a8270fdebc9",
   "metadata": {},
   "source": [
    "Applying Convolution Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2a520ce-d681-46df-83df-785d758099c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hp\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Conv2D\n",
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64,64,3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5af90b2-5c58-475c-a64f-468bd8187a56",
   "metadata": {},
   "source": [
    "Adding the Pooling layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de351040-62ce-4a79-93c0-f26818a8181d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import BatchNormalization\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
    "cnn.add(BatchNormalization())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7f5019-2055-4c2b-8ecd-57cd4704ceae",
   "metadata": {},
   "source": [
    "Applying the second convolution layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96ff034b-4d06-4678-a27d-47ef801f6dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2785bb-a576-4bf6-afb8-6bf60b5760ad",
   "metadata": {},
   "source": [
    "Applying Second Pooling Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1809fadf-087d-4db5-b5b5-04a54ce5baca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchNormalization name=batch_normalization_1, built=False>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
    "BatchNormalization()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6e74c2-482c-4df6-90ca-00f37bde1b8d",
   "metadata": {},
   "source": [
    "Adding Third Convolution Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3498b740-262f-4774-8b9b-08fb4ec07a44",
   "metadata": {},
   "source": [
    "Flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2804ddf7-d3de-42cc-b206-86923fa1ffc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22d3472-eab4-4691-b1b3-7b004ad0d147",
   "metadata": {},
   "source": [
    "Fully Connected Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "00f78c66-dbf3-4656-8a8e-1e83af6b4f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb440c0-11b4-48fe-b10c-7b100bdd4508",
   "metadata": {},
   "source": [
    "Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "967383e6-e752-4f1d-bd28-b304a296df5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2c3b9b80-6e0a-44d2-b5b7-56e54f0a11c7",
   "metadata": {},
   "source": [
    "Compiling  the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e072fc2b-0d7d-4e37-a334-35affabcb870",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "cnn.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69003cff-b00d-439f-a583-a5b226da6fcf",
   "metadata": {},
   "source": [
    "Training the Cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5a398166-fa49-4562-bb3a-3a1e268b28c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hp\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m237s\u001b[0m 1s/step - accuracy: 0.7652 - loss: 0.5774 - val_accuracy: 0.6250 - val_loss: 0.7382\n",
      "Epoch 2/20\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m250s\u001b[0m 1s/step - accuracy: 0.8650 - loss: 0.3047 - val_accuracy: 0.6250 - val_loss: 1.1532\n",
      "Epoch 3/20\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m192s\u001b[0m 1s/step - accuracy: 0.8754 - loss: 0.2960 - val_accuracy: 0.6971 - val_loss: 0.6671\n",
      "Epoch 4/20\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m207s\u001b[0m 1s/step - accuracy: 0.8907 - loss: 0.2577 - val_accuracy: 0.7067 - val_loss: 0.5274\n",
      "Epoch 5/20\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m207s\u001b[0m 1s/step - accuracy: 0.9026 - loss: 0.2419 - val_accuracy: 0.6923 - val_loss: 0.6380\n",
      "Epoch 6/20\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m183s\u001b[0m 1s/step - accuracy: 0.9040 - loss: 0.2216 - val_accuracy: 0.8077 - val_loss: 0.3780\n",
      "Epoch 7/20\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 1s/step - accuracy: 0.9125 - loss: 0.2133 - val_accuracy: 0.7420 - val_loss: 0.8615\n",
      "Epoch 8/20\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 1s/step - accuracy: 0.9069 - loss: 0.2145 - val_accuracy: 0.8381 - val_loss: 0.3531\n",
      "Epoch 9/20\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m192s\u001b[0m 1s/step - accuracy: 0.9201 - loss: 0.1984 - val_accuracy: 0.6635 - val_loss: 1.8792\n",
      "Epoch 10/20\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m212s\u001b[0m 1s/step - accuracy: 0.9190 - loss: 0.2010 - val_accuracy: 0.8718 - val_loss: 0.3031\n",
      "Epoch 11/20\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m233s\u001b[0m 1s/step - accuracy: 0.9216 - loss: 0.1949 - val_accuracy: 0.8205 - val_loss: 0.3706\n",
      "Epoch 12/20\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m438s\u001b[0m 3s/step - accuracy: 0.9257 - loss: 0.1890 - val_accuracy: 0.8766 - val_loss: 0.2877\n",
      "Epoch 13/20\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m209s\u001b[0m 1s/step - accuracy: 0.9243 - loss: 0.1844 - val_accuracy: 0.5994 - val_loss: 1.1109\n",
      "Epoch 14/20\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 1s/step - accuracy: 0.9173 - loss: 0.2083 - val_accuracy: 0.8830 - val_loss: 0.3267\n",
      "Epoch 15/20\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 1s/step - accuracy: 0.9325 - loss: 0.1752 - val_accuracy: 0.8878 - val_loss: 0.2900\n",
      "Epoch 16/20\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 1s/step - accuracy: 0.9276 - loss: 0.1846 - val_accuracy: 0.8654 - val_loss: 0.3001\n",
      "Epoch 17/20\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 1s/step - accuracy: 0.9246 - loss: 0.1933 - val_accuracy: 0.8734 - val_loss: 0.4126\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "history = cnn.fit(\n",
    "    train_set,\n",
    "    validation_data=test_set,\n",
    "    epochs=20,\n",
    "    callbacks=[early_stopping]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfc06f7-79b6-4127-a34c-24afb6eb7476",
   "metadata": {},
   "source": [
    "Evaluate on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d2c585b2-b7bf-48e3-8847-c3eb885ac4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 788ms/step - accuracy: 0.8682 - loss: 0.3094\n",
      "Test Accuracy: 0.8766\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = cnn.evaluate(test_set)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9056984b-9e8a-43e1-bf2b-556f1c2ebd47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 250ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "test_image=image.load_img(r'E:\\AI\\chest_xray\\val\\NORMAL\\NORMAL2-IM-1431-0001.jpeg', target_size=(64,64))\n",
    "#image to array function\n",
    "test_image= image.img_to_array(test_image)/255.0\n",
    "test_image= np.expand_dims(test_image, axis=0)\n",
    "result= cnn.predict(test_image)\n",
    "train_set.class_indices\n",
    "if result[0][0] > 0.5:\n",
    "  prediction = 'Penumoia'\n",
    "else:\n",
    "  prediction = 'Normal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c4c95602-9986-4926-856a-9754caa9455e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal\n"
     ]
    }
   ],
   "source": [
    "print(prediction)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
